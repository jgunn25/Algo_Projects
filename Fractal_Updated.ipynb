{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c4e3a5-08a1-4ebf-a4c1-1c78594363d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygon import RESTClient\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b86867-b7ab-4ee9-8668-242c48f85300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"QQQ_3min_2021_2025.csv\")\n",
    "data = df.copy()\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data['date'] = data['timestamp'].dt.date\n",
    "data['time'] = data['timestamp'].dt.time\n",
    "\n",
    "# Filter for regular market hours\n",
    "data = data[(data['time'] >= pd.to_datetime(\"09:30\").time()) & \n",
    "            (data['time'] <= pd.to_datetime(\"16:00\").time())]\n",
    "\n",
    "# Only keep days with exactly 130 rows\n",
    "valid_dates = data['date'].value_counts()\n",
    "valid_dates = valid_dates[valid_dates <= 135].index\n",
    "data = data[data['date'].isin(valid_dates)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57cc1c5-f83d-4d95-a32b-b69405b68ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Filter market hours: 09:30 to 16:00\n",
    "\n",
    "data = data[(data['time'] >= pd.to_datetime(\"09:30\").time()) & (data['time'] <= pd.to_datetime(\"16:00\").time())]\n",
    "\n",
    "# Ensure each day has all expected intervals (26 x 15min = 6.5 hours)\n",
    "full_df = data.groupby(data['timestamp'].dt.date).filter(lambda x: len(x) > 20)\n",
    "\n",
    "# Drop temporary column\n",
    "full_df.drop(columns='time', inplace=True)\n",
    "\n",
    "# Result stored in `full_days`\n",
    "full_df['date'] = full_df['timestamp'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0156600-92f0-4c87-8295-ed4bc92a1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from dtaidistance import dtw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "class fractal_analysis:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy() \n",
    "        self.df['date'] = self.df['timestamp'].dt.date # Creates a date time stamp feature column\n",
    "        self.ref_df_scaled = None # the reference days list\n",
    "        self.ref_dates = [] # Holds list of dates used for the refernce pattern. \n",
    "        self.ref_len = 0 # length of reference dataframe\n",
    "        self.result_df = {} # holds the results from the CV test\n",
    "\n",
    "    def _prepare_day(self, date):\n",
    "        # Ensures data is in datetime.date format.\n",
    "        d = pd.to_datetime(date).date()\n",
    "\n",
    "        #  Filters rows for the given date and within market hours\n",
    "        day_df = self.df[\n",
    "            (self.df['timestamp'].dt.date == d) &\n",
    "            (self.df['timestamp'].dt.time >= pd.to_datetime(\"09:30\").time()) &\n",
    "            (self.df['timestamp'].dt.time < pd.to_datetime(\"16:00\").time())\n",
    "        ]\n",
    "\n",
    "        # Raise error if day is missing\n",
    "        if day_df.empty:\n",
    "            raise ValueError(f\"No data found for {date}\")\n",
    "            \n",
    "        return day_df['close'].values.reshape(-1, 1) # returns a 2D array\n",
    "\n",
    "    def set_reference_days(self, dates: list):\n",
    "\n",
    "        # Convert input string dates to datetime.date objects. \n",
    "        self.reference_dates = [pd.to_datetime(d).date() for d in dates]\n",
    "\n",
    "        # Collect all data for each date and flatten them into one series\n",
    "        y_segments = [self._prepare_day(d) for d in self.reference_dates]\n",
    "        y_combined = np.concatenate(y_segments).flatten()\n",
    "\n",
    "        # Check if there's enough data. \n",
    "        if len(y_combined) < 2:\n",
    "            raise ValueError(\"Not enough data in reference days.\")\n",
    "\n",
    "        # normalize so first value is 100.\n",
    "        self.ref_df_scaled = (y_combined / y_combined[0]) * 100\n",
    "        self.ref_len = len(self.ref_df_scaled)\n",
    "\n",
    "    \n",
    "    def shape_loss(self, start_date: str, chunk_size: int) -> dict:\n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "        d0 = pd.to_datetime(start_date).date()\n",
    "        idx = all_dates.index(d0)\n",
    "        test_dates = all_dates[idx:idx + chunk_size]\n",
    "    \n",
    "        y_segments = [self._prepare_day(d) for d in test_dates]\n",
    "        y_combined = np.concatenate(y_segments).flatten()  # Ensure 1-D\n",
    "    \n",
    "        if len(y_combined) < 2:\n",
    "            raise ValueError(\"Not enough data in test days.\")\n",
    "    \n",
    "        y_scaled = (y_combined / y_combined[0]) * 100  # Normalize to start=100\n",
    "    \n",
    "        ref = self.ref_df_scaled[:len(y_scaled)].flatten()  # Ensure 1-D\n",
    "        test = y_scaled[:len(ref)].flatten()  # Ensure 1-D\n",
    "    \n",
    "        dtw_loss, _ = fastdtw(ref, test, dist=euclidean)\n",
    "        ols_loss = mean_squared_error(ref, test)\n",
    "    \n",
    "        return {'dtw_loss': dtw_loss}\n",
    "      \n",
    "    def CV_test(self, chunk_size: int = 1):\n",
    "        print(f\"Testing Started for chunk size {chunk_size}\")\n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "        results = []\n",
    "    \n",
    "        for i in range(len(all_dates) - chunk_size):\n",
    "            start_date = all_dates[i]\n",
    "            if start_date in self.ref_dates:\n",
    "                continue\n",
    "            try:\n",
    "                loss = self.shape_loss(start_date, chunk_size=chunk_size)\n",
    "                results.append({'start_date': start_date, 'loss': loss})\n",
    "            except Exception as e:\n",
    "                print(f\"Error on {start_date}: {e}\")\n",
    "                continue\n",
    "    \n",
    "        self.result_df = pd.DataFrame(results).sort_values(\n",
    "            by=lambda x: (x['loss']['dtw_loss'], x['loss']['ols_loss'])\n",
    "        )\n",
    "        print(\"Testing Done\")\n",
    "\n",
    "    def run_multiple_chunks(self, chunk_sizes=[1, 5, 30]):\n",
    "        for chunk in chunk_sizes:\n",
    "            print(f\"\\n--- Chunk Size: {chunk} ---\")\n",
    "            self.CV_test(chunk_size=chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d44c275-934a-43a8-b8ea-d8d26c61ce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2025, 5, 27) datetime.date(2025, 5, 28)\n",
      " datetime.date(2025, 5, 29)]\n",
      "Running CV for chunk size 1\n",
      "Error on 2023-01-03: Input vector should be 1-D.\n",
      "Error on 2023-01-04: Input vector should be 1-D.\n",
      "Error on 2023-01-05: Input vector should be 1-D.\n",
      "Error on 2023-01-06: Input vector should be 1-D.\n",
      "Error on 2023-01-09: Input vector should be 1-D.\n",
      "Error on 2023-01-10: Input vector should be 1-D.\n",
      "Error on 2023-01-11: Input vector should be 1-D.\n",
      "Error on 2023-01-12: Input vector should be 1-D.\n",
      "Error on 2023-01-13: Input vector should be 1-D.\n",
      "Error on 2023-01-17: Input vector should be 1-D.\n",
      "Error on 2023-01-18: Input vector should be 1-D.\n",
      "Error on 2023-01-19: Input vector should be 1-D.\n",
      "Error on 2023-01-20: Input vector should be 1-D.\n",
      "Error on 2023-01-23: Input vector should be 1-D.\n",
      "Error on 2023-01-24: Input vector should be 1-D.\n",
      "Error on 2023-01-25: Input vector should be 1-D.\n",
      "Error on 2023-01-26: Input vector should be 1-D.\n",
      "Error on 2023-01-27: Input vector should be 1-D.\n",
      "Error on 2023-01-30: Input vector should be 1-D.\n",
      "Error on 2023-01-31: Input vector should be 1-D.\n",
      "Error on 2023-02-01: Input vector should be 1-D.\n",
      "Error on 2023-02-02: Input vector should be 1-D.\n",
      "Error on 2023-02-03: Input vector should be 1-D.\n",
      "Error on 2023-02-06: Input vector should be 1-D.\n",
      "Error on 2023-02-07: Input vector should be 1-D.\n",
      "Error on 2023-02-08: Input vector should be 1-D.\n",
      "Error on 2023-02-09: Input vector should be 1-D.\n",
      "Error on 2023-02-10: Input vector should be 1-D.\n",
      "Error on 2023-02-13: Input vector should be 1-D.\n",
      "Error on 2023-02-14: Input vector should be 1-D.\n",
      "Error on 2023-02-15: Input vector should be 1-D.\n",
      "Error on 2023-02-16: Input vector should be 1-D.\n",
      "Error on 2023-02-17: Input vector should be 1-D.\n",
      "Error on 2023-02-21: Input vector should be 1-D.\n",
      "Error on 2023-02-22: Input vector should be 1-D.\n",
      "Error on 2023-02-23: Input vector should be 1-D.\n",
      "Error on 2023-02-24: Input vector should be 1-D.\n",
      "Error on 2023-02-27: Input vector should be 1-D.\n",
      "Error on 2023-02-28: Input vector should be 1-D.\n",
      "Error on 2023-03-01: Input vector should be 1-D.\n",
      "Error on 2023-03-02: Input vector should be 1-D.\n",
      "Error on 2023-03-03: Input vector should be 1-D.\n",
      "Error on 2023-03-06: Input vector should be 1-D.\n",
      "Error on 2023-03-07: Input vector should be 1-D.\n",
      "Error on 2023-03-08: Input vector should be 1-D.\n",
      "Error on 2023-03-09: Input vector should be 1-D.\n",
      "Error on 2023-03-10: Input vector should be 1-D.\n",
      "Error on 2023-03-13: Input vector should be 1-D.\n",
      "Error on 2023-03-14: Input vector should be 1-D.\n",
      "Error on 2023-03-15: Input vector should be 1-D.\n",
      "Error on 2023-03-16: Input vector should be 1-D.\n",
      "Error on 2023-03-17: Input vector should be 1-D.\n",
      "Error on 2023-03-20: Input vector should be 1-D.\n",
      "Error on 2023-03-21: Input vector should be 1-D.\n",
      "Error on 2023-03-22: Input vector should be 1-D.\n",
      "Error on 2023-03-23: Input vector should be 1-D.\n",
      "Error on 2023-03-24: Input vector should be 1-D.\n",
      "Error on 2023-03-27: Input vector should be 1-D.\n",
      "Error on 2023-03-28: Input vector should be 1-D.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m fa \u001b[38;5;241m=\u001b[39m FractalAnalysis(short_df)\n\u001b[0;32m      7\u001b[0m fa\u001b[38;5;241m.\u001b[39mset_reference_days(ref_date)\n\u001b[1;32m----> 8\u001b[0m fa\u001b[38;5;241m.\u001b[39mrun_cv(chunk_sizes\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[61], line 52\u001b[0m, in \u001b[0;36mFractalAnalysis.run_cv\u001b[1;34m(self, chunk_sizes)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     segments \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_day_data(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m chunk]\n\u001b[0;32m     53\u001b[0m     combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(segments)\n\u001b[0;32m     54\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m (combined \u001b[38;5;241m/\u001b[39m combined[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[1;32mIn[61], line 21\u001b[0m, in \u001b[0;36mFractalAnalysis._get_day_data\u001b[1;34m(self, date)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_day_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, date):\n\u001b[0;32m     18\u001b[0m     d \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(date)\u001b[38;5;241m.\u001b[39mdate()\n\u001b[0;32m     19\u001b[0m     day_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\n\u001b[0;32m     20\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate \u001b[38;5;241m==\u001b[39m d) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m---> 21\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m09:30\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtime()) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     22\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m<\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m16:00\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m     23\u001b[0m     ]\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m day_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\accessor.py:96\u001b[0m, in \u001b[0;36mPandasDelegate._add_delegate_accessors.<locals>._create_delegator_property.<locals>._getter\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getter\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_delegate_property_get(name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\accessors.py:93\u001b[0m, in \u001b[0;36mProperties._delegate_property_get\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[0;32m     91\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values()\n\u001b[1;32m---> 93\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(values, name)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# maybe need to upcast (ints)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\extension.py:68\u001b[0m, in \u001b[0;36m_inherit_from_data.<locals>.fget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfget\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 68\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, name)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ref_date = np.unique(full_df['date'])[-4:-1]\n",
    "print(ref_date)\n",
    "\n",
    "short_df =  full_df[(full_df['timestamp'] >= '2023-01-01') ]\n",
    "\n",
    "fa = FractalAnalysis(short_df)\n",
    "fa.set_reference_days(ref_date)\n",
    "fa.run_cv(chunk_sizes= [1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a75e5a6b-6b3c-45ba-93e0-a1df14315340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fractal_analysis:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df['date'] = self.df['timestamp'].dt.date\n",
    "        self.reference_day = None\n",
    "        self.reference_dates = []\n",
    "        self.reference_day_len = 0\n",
    "        self.result_df = None\n",
    "\n",
    "    def _prepare_day(self, date):\n",
    "        d = pd.to_datetime(date).date()\n",
    "        day_df = self.df[\n",
    "            (self.df['timestamp'].dt.date == d) &\n",
    "            (self.df['timestamp'].dt.time >= pd.to_datetime(\"09:30\").time()) &\n",
    "            (self.df['timestamp'].dt.time < pd.to_datetime(\"16:00\").time())\n",
    "        ]\n",
    "        if day_df.empty:\n",
    "            raise ValueError(f\"No data found for {date}\")\n",
    "        return day_df['close'].values.reshape(-1, 1)\n",
    "\n",
    "    def HIDE_set_reference_days(self, dates: list):\n",
    "        self.reference_dates = [pd.to_datetime(d).date() for d in dates]\n",
    "        y_segments = [self._prepare_day(d) for d in self.reference_dates]\n",
    "        y_combined = np.concatenate(y_segments)\n",
    "        self.reference_day_len = len(y_combined)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "        self.reference_day = scaler.fit_transform(y_combined).flatten()\n",
    "\n",
    "    def set_reference_days(self, dates: list):\n",
    "        self.reference_dates = [pd.to_datetime(d).date() for d in dates]\n",
    "        y_segments = [self._prepare_day(d) for d in self.reference_dates]\n",
    "        y_combined = np.concatenate(y_segments).flatten()\n",
    "    \n",
    "        if len(y_combined) < 2:\n",
    "            raise ValueError(\"Not enough data in reference days.\")\n",
    "\n",
    "        self.reference_day = (y_combined / y_combined[0]) * 100\n",
    "\n",
    "    def HIDE_shape_loss(self, start_date: str) -> float:\n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "        d0 = pd.to_datetime(start_date).date()\n",
    "\n",
    "        if d0 not in all_dates:\n",
    "            raise ValueError(f\"{d0} not found in data.\")\n",
    "\n",
    "        idx = all_dates.index(d0)\n",
    "        if idx + len(self.reference_dates) > len(all_dates):\n",
    "            raise ValueError(f\"Not enough following dates after {d0}.\")\n",
    "\n",
    "        test_dates = all_dates[idx:idx + len(self.reference_dates)]\n",
    "        y_segments = [self._prepare_day(d) for d in test_dates]\n",
    "        y_combined = np.concatenate(y_segments)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "        y_scaled = scaler.fit_transform(y_combined).flatten()\n",
    "\n",
    "        min_len = min(len(self.reference_day), len(y_scaled))\n",
    "        ref = self.reference_day[:min_len]\n",
    "        test = y_scaled[:min_len]\n",
    "\n",
    "        return mean_squared_error(ref, test)\n",
    "\n",
    "    def shape_loss(self, start_date: str) -> float:\n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "        d0 = pd.to_datetime(start_date).date()\n",
    "        idx = all_dates.index(d0)\n",
    "        test_dates = all_dates[idx:idx + len(self.reference_dates)]\n",
    "    \n",
    "        y_segments = [self._prepare_day(d) for d in test_dates]\n",
    "        y_combined = np.concatenate(y_segments).flatten()\n",
    "    \n",
    "        if len(y_combined) < 2:\n",
    "            raise ValueError(\"Not enough data in test days.\")\n",
    "    \n",
    "        y_scaled = (y_combined / y_combined[0]) * 100\n",
    "        ref = self.reference_day[:len(y_scaled)]\n",
    "        test = y_scaled[:len(ref)]\n",
    "    \n",
    "        return mean_squared_error(ref, test)\n",
    "\n",
    "    \n",
    "    def CV_test(self):\n",
    "        print(\"Testing Started\")\n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "        results = []\n",
    "\n",
    "        for i in range(len(all_dates) - len(self.reference_dates)):\n",
    "            start_date = all_dates[i]\n",
    "            if start_date in self.reference_dates:\n",
    "                continue\n",
    "            try:\n",
    "                loss = self.shape_loss(start_date)\n",
    "                results.append({'start_date': start_date, 'loss': loss})\n",
    "            except Exception as e:\n",
    "                print(f\"Error on {start_date}: {e}\")\n",
    "                continue\n",
    "\n",
    "        self.result_df = pd.DataFrame(results).sort_values(by='loss')\n",
    "        print(\"Testing Done\")\n",
    "\n",
    "    def HIDE_plot(self, start_date: str, lookahead_days: int = 1):\n",
    "        if self.reference_day is None:\n",
    "            raise ValueError(\"Reference day not set. Call set_reference_days() first.\")\n",
    "    \n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "        d0 = pd.to_datetime(start_date).date()\n",
    "        if d0 not in all_dates:\n",
    "            raise ValueError(f\"Start date {d0} not found.\")\n",
    "    \n",
    "        idx = all_dates.index(d0)\n",
    "        test_span = len(self.reference_dates)\n",
    "        total_needed = test_span + lookahead_days\n",
    "    \n",
    "        if idx + total_needed > len(all_dates):\n",
    "            raise ValueError(f\"Not enough days after {start_date} to include {lookahead_days} future day(s).\")\n",
    "    \n",
    "        test_dates = all_dates[idx:idx + total_needed]\n",
    "        y_segments = [self._prepare_day(d) for d in test_dates]\n",
    "        y_combined = np.concatenate(y_segments)\n",
    "    \n",
    "        scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "        y_scaled = scaler.fit_transform(y_combined).flatten()\n",
    "    \n",
    "        match_len = sum(len(s) for s in y_segments[:test_span])\n",
    "        ref_len = min(len(self.reference_day), match_len)\n",
    "        X_ref = np.arange(ref_len)\n",
    "        X_full = np.arange(len(y_scaled))\n",
    "    \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(X_ref, self.reference_day[:ref_len], label=f'Reference: {\" + \".join(str(d) for d in self.reference_dates)}', linewidth=1)\n",
    "        plt.plot(X_full, y_scaled, '--', label=f'Test: {test_dates[0]} + {test_span}d + {lookahead_days}d future', linewidth=2)\n",
    "        plt.axvline(ref_len, color='red', linestyle=':', label='Future start')\n",
    "        plt.title(f'Fractal Shape Comparison\\nRef: {self.reference_dates} vs Test: {test_dates[:test_span]} + {lookahead_days} future day(s)')\n",
    "        plt.xlabel(\"3-min intervals\")\n",
    "        plt.ylabel(\"Scaled Price\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_individual(self, lookahead_days: int = 1, top_n: int = 20):\n",
    "        if self.reference_day is None or not self.reference_dates:\n",
    "            raise ValueError(\"Reference day not set.\")\n",
    "        if self.result_df is None or self.result_df.empty:\n",
    "            raise ValueError(\"No test results found. Run CV_test() first.\")\n",
    "    \n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "        ref_last_date = max(self.reference_dates)\n",
    "        ref_end_idx = all_dates.index(ref_last_date)\n",
    "    \n",
    "        # Plot reference future if available\n",
    "        ref_future = []\n",
    "        ref_future_dates = all_dates[ref_end_idx + 1 : ref_end_idx + 1 + lookahead_days] \\\n",
    "            if ref_end_idx + lookahead_days < len(all_dates) else []\n",
    "    \n",
    "        if ref_future_dates:\n",
    "            ref_future_segments = [self._prepare_day(d).flatten() for d in ref_future_dates]\n",
    "            ref_future = np.concatenate(ref_future_segments)\n",
    "            ref_future = (ref_future / ref_future[0]) * 100\n",
    "    \n",
    "        for i, row in self.result_df.head(top_n).iterrows():\n",
    "            start_date = pd.to_datetime(row['start_date']).date()\n",
    "            test_start_idx = all_dates.index(start_date)\n",
    "            test_total_days = len(self.reference_dates) + lookahead_days\n",
    "    \n",
    "            if test_start_idx + test_total_days > len(all_dates):\n",
    "                continue\n",
    "    \n",
    "            test_dates = all_dates[test_start_idx : test_start_idx + test_total_days]\n",
    "            y_segments = [self._prepare_day(d).flatten() for d in test_dates]\n",
    "            y_combined = np.concatenate(y_segments)\n",
    "            y_scaled = (y_combined / y_combined[0]) * 100\n",
    "    \n",
    "            # Plot\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            X_ref = np.arange(len(self.reference_day))\n",
    "            plt.plot(X_ref, self.reference_day, label='Reference', linewidth=2, color='black')\n",
    "    \n",
    "            if len(ref_future):\n",
    "                X_ref_future = np.arange(len(self.reference_day), len(self.reference_day) + len(ref_future))\n",
    "                plt.plot(X_ref_future, ref_future, '--', label=f'Ref Future ({ref_future_dates[0]})', color='gray')\n",
    "    \n",
    "            X_test = np.arange(len(y_scaled))\n",
    "            plt.plot(X_test, y_scaled, label=f'Test: {start_date} (+{lookahead_days}d)', alpha=0.8, linewidth=2)\n",
    "    \n",
    "            plt.axvline(len(self.reference_day), color='red', linestyle=':', label='Future starts')\n",
    "            plt.title(f'Match {i+1}: Ref vs Test {start_date}')\n",
    "            plt.xlabel('3-min intervals')\n",
    "            plt.ylabel('Indexed Price (Start = 100)')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "             \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d823a5d-fa47-4d18-bca0-31aac5d6c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FractalAnalysis:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df['date'] = self.df['timestamp'].dt.date\n",
    "        self.ref_dates = []\n",
    "        self.ref_data = None\n",
    "        self.ref_len = 0\n",
    "        self.results = {}\n",
    "\n",
    "    def _get_day_data(self, date):\n",
    "        d = pd.to_datetime(date).date()\n",
    "        day_df = self.df[\n",
    "            (self.df['timestamp'].dt.date == d) &\n",
    "            (self.df['timestamp'].dt.time >= pd.to_datetime(\"09:30\").time()) &\n",
    "            (self.df['timestamp'].dt.time < pd.to_datetime(\"16:00\").time())\n",
    "        ]\n",
    "        if day_df.empty:\n",
    "            raise ValueError(f\"No data found for {date}\")\n",
    "        return day_df['close'].values.flatten()  # ensure 1D array\n",
    "\n",
    "    def set_reference_days(self, dates):\n",
    "        self.ref_dates = [pd.to_datetime(d).date() for d in dates]\n",
    "        segments = [self._get_day_data(d) for d in self.ref_dates]\n",
    "        combined = np.concatenate(segments)\n",
    "        self.ref_data = (combined / combined[0]) * 100\n",
    "        self.ref_len = len(self.ref_data)\n",
    "\n",
    "    def _dtw_loss(self, test_data):\n",
    "        ref_resized = np.interp(np.linspace(0, len(self.ref_data), num=len(test_data)),\n",
    "                                np.arange(len(self.ref_data)), self.ref_data)\n",
    "        return fastdtw(ref_resized, test_data, dist=euclidean)[0]\n",
    "\n",
    "    def run_cv(self, chunk_sizes):\n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "        ref_date_set = set(self.ref_dates)\n",
    "\n",
    "        for size in chunk_sizes:\n",
    "            print(f\"Running CV for chunk size {size}\")\n",
    "            results = []\n",
    "            for i in range(len(all_dates) - size):\n",
    "                chunk = all_dates[i:i+size]\n",
    "                if any(d in ref_date_set for d in chunk):\n",
    "                    continue\n",
    "                try:\n",
    "                    segments = [self._get_day_data(d) for d in chunk]\n",
    "                    combined = np.concatenate(segments)\n",
    "                    scaled = (combined / combined[0]) * 100\n",
    "                    scaled = scaled.flatten()  # ensure 1D array\n",
    "                    loss = self._dtw_loss(scaled)\n",
    "                    results.append({\"start_date\": chunk[0], \"loss\": loss})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error on {chunk[0]}: {e}\")\n",
    "                    continue\n",
    "            self.results[size] = pd.DataFrame(results).sort_values(by=\"loss\")\n",
    "\n",
    "    def plot_top_matches(self, size, top_n=5):\n",
    "        if size not in self.results:\n",
    "            raise ValueError(\"Chunk size not found in results\")\n",
    "\n",
    "        df = self.results[size].head(top_n)\n",
    "        all_dates = sorted(self.df['date'].unique())\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            start_date = pd.to_datetime(row['start_date']).date()\n",
    "            idx = all_dates.index(start_date)\n",
    "            dates = all_dates[idx:idx+size]\n",
    "            segments = [self._get_day_data(d) for d in dates]\n",
    "            combined = np.concatenate(segments)\n",
    "            scaled = (combined / combined[0]) * 100\n",
    "            scaled = scaled.flatten()\n",
    "\n",
    "            ref = self.ref_data\n",
    "            test = np.interp(np.linspace(0, len(ref), num=len(scaled)), np.arange(len(ref)), ref)\n",
    "\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(np.arange(len(ref)), ref, label='Reference', linewidth=2)\n",
    "            plt.plot(np.arange(len(test)), scaled, '--', label=f'Test: {start_date}', linewidth=1.5)\n",
    "            plt.title(f\"Match: {start_date}, DTW Loss: {row['loss']:.2f}\")\n",
    "            plt.xlabel(\"3-min intervals\")\n",
    "            plt.ylabel(\"Scaled Price\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18319dc9-87b0-4149-a374-d6f8a59553ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c0c24-70e5-4f91-8c98-859b81ff98ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d6a95-2680-47f2-92ed-216ad88e9000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
